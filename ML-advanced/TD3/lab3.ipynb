{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the advanced Machine Learning Course.\n",
    "\n",
    "The objective of this lab session is to code a few regression algorithms and to apply them to synthetic and real datasets.\n",
    "\n",
    "Please put **\"ML - MDS - TD3\"** in the mail subject or I might lose your work (which means 0) and send it to pierre.houdouin@centralesupelec.fr\n",
    "\n",
    "Please label your notebook **\"L3_familyname1_familyname2.ipynb\"** or I might lose your work (which means 0).\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the dataset that we are going to use, an indian dataset including in the last column information about the diabetes status of patients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "data = pd.read_csv(\"w8a.csv\", sep=\";\", header=None)\n",
    "\n",
    "X = data.iloc[:,:-1].to_numpy()\n",
    "y = data.iloc[:,-1].to_numpy()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll be moving from linear regression to logistic regression, one of the simplest ways to deal with a classification problem. Instead of fitting a line, logistic regression models the probability that the outcome is 1 given the value of the predictor. In order to do this we need a function that transforms our predictor variable to a value between 0 and 1. Lots of functions can do that, but the logistic function is the most common choice:\n",
    "\n",
    "$$f(z) = \\frac{1}{1+\\exp{-z}}.$$\n",
    "\n",
    "To predict the class of our observations we'll have to minimize the corresponding loss function and as we are in a high-dimensional context we'll add an $l_2$ regularization to the model:\n",
    "\n",
    "$$L(\\textbf{w}) = \\sum_{i=1}^n log(1+\\exp(-y_i\\textbf{w}^Tx_i))+\\frac{\\lambda}{2} \\| \\textbf{w} \\|^2,$$\n",
    "\n",
    "where $x_i$ is the vector of features for the observation $i$ and $y_i \\in \\{-1, 1\\}$ is the class label.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use the `sklearn` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=32, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we compute the accuracy score to evaluate the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "Implement from scratch your own logistic regression model with stochastic gradient descent optimization. \n",
    "\n",
    "- Fill in the class\n",
    "\n",
    "- Display the evolution of the cost function along iterations. Do this for several strategies for the setting of the learning rate\n",
    "\n",
    "- Try the different acceleration strategies\n",
    "\n",
    "- Train the model with the training set and evaluate its performance in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "class StochasticLogisticRegression():\n",
    "    \n",
    "    \"\"\" Class for logistic regression:\n",
    "    \n",
    "    Attributes:                                                                                 \n",
    "    -----------                                                                              Default value\n",
    "    coef_         : 1-dimensional np.array, coefficients / weights                         | None                \n",
    "    lambd_        : float,                  regularization parameter                       | 0.1\n",
    "    lr_           : float,                  the learning rate                              | 0.01\n",
    "    bsize         : integer,                the size of the mini-batch >=1                 | 64\n",
    "    gamma         : float,                  gamma coefficient                              | 0.999\n",
    "    beta          : float,                  beta coefficient                               | 0.9\n",
    "    eps           : float,                  epsilon coefficient                            | 10-8\n",
    "    debias        : boolean,                indicates if we use the debiais correction     | False\n",
    "    coef_history_ : list,                   the list of all visited betas/ weights         | []\n",
    "    f_history_    : list ,                  the list of all evaluations in visited betas   | []\n",
    "    thresh        : float,                  decision threshold for classification          | 0.5\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambd=0.1, lr=1e-2, batchsize=64, gamma=0.999, beta=0.9, eps=1e-8, debias=False,\n",
    "                 thresh=0.5):\n",
    "        self.coef_         = None # weights\n",
    "        self.lambd_        = lambd\n",
    "        self.lr_           = lr\n",
    "        self.bsize_        = batchsize\n",
    "        self.gamma_        = gamma\n",
    "        self.beta_         = beta\n",
    "        self.eps_          = eps\n",
    "        self.debias_       = debias\n",
    "        self.coef_history_ = []\n",
    "        self.f_history_    = []\n",
    "        self.thresh_       = thresh\n",
    "\n",
    "    def logistic(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        z = X.dot(self.coef_)\n",
    "        return np.mean(np.log(1 + np.exp(-y * z))) + 0.5 * self.lambd_ * np.sum(self.coef_ ** 2)\n",
    "        \n",
    "    def fit(self, X, y, lr=1e-2, bsize=64, max_iter=100, minibatch=False):\n",
    "        \n",
    "        \"\"\" Fit the data (X, y).\n",
    "    \n",
    "        Parameters:\n",
    "        -----------                                                                         Default value\n",
    "        X          : (num_samples, num_features) np.array, Design matrix                  | \n",
    "        y          : (num_sampes, ) np.array,              Output vector                  | \n",
    "        lr         : float,                                the learning rate              | 0.001\n",
    "        bsize      : integer,                              the size of the mini-batch >=1 | 64\n",
    "        max_iter   : integer,                              the number of epochs           | 100\n",
    "        mini_batch : bool,                                 method used                    | False\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.coef_ = np.zeros(n_features)\n",
    "        m = np.zeros_like(self.coef_)\n",
    "        v = np.zeros_like(self.coef_)\n",
    "        t = 0\n",
    "\n",
    "        for epoch in range(max_iter):\n",
    "            if minibatch:\n",
    "                X, y = shuffle(X, y)\n",
    "                for i in range(0, n_samples, bsize):\n",
    "                    X_batch = X[i:i+bsize]\n",
    "                    y_batch = y[i:i+bsize]\n",
    "                    \n",
    "                    z = X_batch.dot(self.coef_)\n",
    "                    h = self.logistic(z)\n",
    "                    grad = X_batch.T.dot(h - y_batch) / bsize + self.lambd_ * self.coef_\n",
    "                    \n",
    "                    t += 1\n",
    "                    m = self.beta_ * m + (1 - self.beta_) * grad\n",
    "                    v = self.gamma_ * v + (1 - self.gamma_) * (grad ** 2)\n",
    "                    \n",
    "                    if self.debias_:\n",
    "                        m_hat = m / (1 - self.beta_ ** t)\n",
    "                        v_hat = v / (1 - self.gamma_ ** t)\n",
    "                    else:\n",
    "                        m_hat, v_hat = m, v\n",
    "                    \n",
    "                    self.coef_ -= lr * m_hat / (np.sqrt(v_hat) + self.eps_)\n",
    "            else:\n",
    "                z = X.dot(self.coef_)\n",
    "                h = self.logistic(z)\n",
    "                grad = X.T.dot(h - y) / n_samples + self.lambd_ * self.coef_\n",
    "                \n",
    "                t += 1\n",
    "                m = self.beta_ * m + (1 - self.beta_) * grad\n",
    "                v = self.gamma_ * v + (1 - self.gamma_) * (grad ** 2)\n",
    "                \n",
    "                if self.debias_:\n",
    "                    m_hat = m / (1 - self.beta_ ** t)\n",
    "                    v_hat = v / (1 - self.gamma_ ** t)\n",
    "                else:\n",
    "                    m_hat, v_hat = m, v\n",
    "                \n",
    "                self.coef_ -= lr * m_hat / (np.sqrt(v_hat) + self.eps_)\n",
    "            \n",
    "            self.coef_history_.append(self.coef_.copy())\n",
    "            self.f_history_.append(self.loss(X, y))\n",
    "\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Make binary predictions for data X.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array, Design matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        y_pred: (num_samples, ) np.array, Predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        probas = self.logistic(X.dot(self.coef_))\n",
    "        return (probas >= self.thresh_).astype(int)\n",
    "    \n",
    "    def accuracy_evaluation(self,X,y,thresh=0.5,margin=0, vizualisation=True):\n",
    "        \"\"\" Compute detailed accuracy rates.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------                                                              Default value\n",
    "        X                : (num_samples, num_features) np.array, Input data    | \n",
    "        y                : (num_samples, ) np.array, Labels of the input data  | \n",
    "        thresh           : Decision threshold                                  | 0.5\n",
    "        margin           : If the prediction is at a distance less than margin | 0\n",
    "                           to the treshold, returns undetermined label.        |\n",
    "        vizualisation    : Allow the user to vizualize in a board the results  | True\n",
    "            \n",
    "        Returns:\n",
    "        -----\n",
    "        good_prediction  : % of correct classifications\n",
    "        undetermined     : % of indetermined labels \n",
    "        wrong_prediction : % of wrong classifications\n",
    "        TP               : % of 1 labelled 1\n",
    "        UP               : % of 1 labelled undetermined\n",
    "        FN               : % of 1 labelled 0\n",
    "        TN               : % of 0 labelled 0\n",
    "        UN               : % of 0 labelled undetermined\n",
    "        FP               : % of 0 labelled 1\n",
    "        F1_score         : F1-score\n",
    "        \"\"\"  \n",
    "        probas = self.logistic(X.dot(self.coef_))\n",
    "        y_pred = np.zeros_like(y)\n",
    "        y_pred[probas > thresh + margin] = 1\n",
    "        y_pred[probas < thresh - margin] = 0\n",
    "        y_pred[(probas >= thresh - margin) & (probas <= thresh + margin)] = -1  # Undetermined\n",
    "\n",
    "        TP = np.sum((y == 1) & (y_pred == 1)) / np.sum(y == 1)\n",
    "        UP = np.sum((y == 1) & (y_pred == -1)) / np.sum(y == 1)\n",
    "        FN = np.sum((y == 1) & (y_pred == 0)) / np.sum(y == 1)\n",
    "        TN = np.sum((y == 0) & (y_pred == 0)) / np.sum(y == 0)\n",
    "        UN = np.sum((y == 0) & (y_pred == -1)) / np.sum(y == 0)\n",
    "        FP = np.sum((y == 0) & (y_pred == 1)) / np.sum(y == 0)\n",
    "\n",
    "        good_prediction = np.mean(y == y_pred)\n",
    "        undetermined = np.mean(y_pred == -1)\n",
    "        wrong_prediction = np.mean((y != y_pred) & (y_pred != -1))\n",
    "\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        F1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        if vizualisation:\n",
    "            print(f\"Good predictions: {good_prediction:.2%}\")\n",
    "            print(f\"Undetermined: {undetermined:.2%}\")\n",
    "            print(f\"Wrong predictions: {wrong_prediction:.2%}\")\n",
    "            print(f\"True Positives: {TP:.2%}\")\n",
    "            print(f\"Undetermined Positives: {UP:.2%}\")\n",
    "            print(f\"False Negatives: {FN:.2%}\")\n",
    "            print(f\"True Negatives: {TN:.2%}\")\n",
    "            print(f\"Undetermined Negatives: {UN:.2%}\")\n",
    "            print(f\"False Positives: {FP:.2%}\")\n",
    "            print(f\"F1-score: {F1_score:.4f}\")\n",
    "\n",
    "        return good_prediction, undetermined, wrong_prediction, TP, UP, FN, TN, UN, FP, F1_score\n",
    "\n",
    "    \n",
    "    def find_thresh(self, X, y, step=0.01,margin=0):\n",
    "        \"\"\"Find the decision threshsold that maximize the f1_score\n",
    "        \n",
    "        Parameters:\n",
    "        -----------                                                    Default value\n",
    "        X      : (num_samples, num_features) np.array, Input data    | \n",
    "        y      : (num_samples, ) np.array, Labels of the input data  | \n",
    "        step   : Decision threshold                                  | 0.5\n",
    "        margin : If the prediction is at a distance less than margin | 0\n",
    "                 to the treshold, returns undetermined label.        |\n",
    "        \"\"\"  \n",
    "        best_thresh = 0\n",
    "        best_f1 = 0\n",
    "        for thresh in np.arange(0, 1 + step, step):\n",
    "            _, _, _, _, _, _, _, _, _, f1_score = self.accuracy_evaluation(X, y, thresh, margin, visualization=False)\n",
    "            if f1_score > best_f1:\n",
    "                best_f1 = f1_score\n",
    "                best_thresh = thresh\n",
    "        self.thresh_ = best_thresh\n",
    "        return best_thresh, best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaie plusieurs entraînements avec différents coefficients de régularisation, on choisit celui qui minimise\n",
    "l'erreur sur le test set. On entraine sur 1000 itérations en batch gradient descent. On garde **lambda = 2**.\n",
    "On essaie ensuite plusieurs learning rates, on choisit le plus grand qui permet d'obtenir un gradient qui converge. On garde **lr = 0.01**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good predictions: 8.36%\n",
      "Undetermined: 8.65%\n",
      "Wrong predictions: 91.29%\n",
      "True Positives: 1.87%\n",
      "Undetermined Positives: 10.86%\n",
      "False Negatives: 87.27%\n",
      "True Negatives: nan%\n",
      "Undetermined Negatives: nan%\n",
      "False Positives: nan%\n",
      "F1-score: 0.0000\n",
      "Accuracy: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bouet\\AppData\\Local\\Temp\\ipykernel_31428\\652998116.py:155: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TN = np.sum((y == 0) & (y_pred == 0)) / np.sum(y == 0)\n",
      "C:\\Users\\bouet\\AppData\\Local\\Temp\\ipykernel_31428\\652998116.py:156: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  UN = np.sum((y == 0) & (y_pred == -1)) / np.sum(y == 0)\n",
      "C:\\Users\\bouet\\AppData\\Local\\Temp\\ipykernel_31428\\652998116.py:157: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FP = np.sum((y == 0) & (y_pred == 1)) / np.sum(y == 0)\n"
     ]
    }
   ],
   "source": [
    "model = StochasticLogisticRegression(2, 0.01, 64)\n",
    "model.fit(X_train, y_train, max_iter=1000)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "model.accuracy_evaluation(X_test, y_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement only one acceleration method and compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of the batchsize impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Study the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Study the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Study Loss and F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta impact\n",
    "Nous allons comparer les paramètres beta pour gamma = 0.999 et batch_size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Study Loss and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ml_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
